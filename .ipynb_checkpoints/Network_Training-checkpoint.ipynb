{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from normalize import Normalize, MapToRange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import *\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n",
      "1000 trajectories\n",
      "ready\n",
      "Amount of testing trajectories:  200 (Batchsize: 256)\n",
      "Amount of training trajectories:  800 (Batchsize: 256)\n"
     ]
    }
   ],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        i, j = self.indices[index]        \n",
    "        X = torch.tensor([\n",
    "            self.dataset['dx'][i, j],\n",
    "            self.dataset['dy'][i, j],\n",
    "            self.dataset['dz'][i, j],\n",
    "            self.dataset['vx'][i, j],\n",
    "            self.dataset['vy'][i, j],\n",
    "            self.dataset['vz'][i, j],\n",
    "            self.dataset['phi'][i, j],\n",
    "            self.dataset['theta'][i, j],\n",
    "            self.dataset['psi'][i, j],\n",
    "            self.dataset['p'][i, j],\n",
    "            self.dataset['q'][i, j],\n",
    "            self.dataset['r'][i, j],\n",
    "            self.dataset['omega'][i, j, 0],\n",
    "            self.dataset['omega'][i, j, 1],\n",
    "            self.dataset['omega'][i, j, 2],\n",
    "            self.dataset['omega'][i, j, 3],\n",
    "#             self.dataset['Mx_ext'][i],\n",
    "#             self.dataset['My_ext'][i],\n",
    "#             self.dataset['Mz_ext'][i]\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        U = torch.tensor([\n",
    "            self.dataset['u'][i, j, 0],\n",
    "            self.dataset['u'][i, j, 1],\n",
    "            self.dataset['u'][i, j, 2],\n",
    "            self.dataset['u'][i, j, 3]\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        return X, U\n",
    "    \n",
    "# trajectories containing 199 points\n",
    "dataset_path = 'datasets/HOVER_TO_HOVER_NOMINAL.npz'\n",
    "\n",
    "dataset = dict()\n",
    "print('loading dataset...')\n",
    "# See all keys and shapes\n",
    "# with np.load(dataset_path) as data:\n",
    "#     for key in data.files:\n",
    "#         print(key, data[key].shape)\n",
    "\n",
    "with np.load(dataset_path) as full_dataset:\n",
    "    # total number of trajectories\n",
    "    num = len(full_dataset['dx'])\n",
    "    print(num, 'trajectories')\n",
    "    dataset = {key: full_dataset[key] for key in [\n",
    "        't', 'dx', 'dy', 'dz', 'vx', 'vy', 'vz', 'phi', 'theta', 'psi', 'p', 'q', 'r','omega', 'u', 'omega_min','omega_max', 'k_omega', 'Mx_ext', 'My_ext', 'Mz_ext'\n",
    "    ]}\n",
    "\n",
    "# train/test split\n",
    "batchsize_train = 256\n",
    "batchsize_val = 256\n",
    "train_trajectories = range(int(0.8*num))\n",
    "test_trajectories = list(set(range(num)) - set(train_trajectories))\n",
    "\n",
    "train_indices = [(i, j) for i in train_trajectories for j in range(199)]\n",
    "train_set = TrajectoryDataset(dataset, train_indices)\n",
    "train_loader = DataLoader(train_set, batch_size=batchsize_train, shuffle=True, num_workers=1)\n",
    "\n",
    "test_indices = [(i, j) for i in test_trajectories for j in range(199)]\n",
    "test_set = TrajectoryDataset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_set, batch_size=batchsize_val, shuffle=True, num_workers=1)\n",
    "\n",
    "print('ready')\n",
    "\n",
    "print('Amount of testing trajectories: ',len(test_trajectories),f'(Batchsize: {batchsize_val})')\n",
    "print('Amount of training trajectories: ',len(train_trajectories),f'(Batchsize: {batchsize_train})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "800\n",
      "[5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500 5500\n",
      " 5500 5500 5500 5500 5500 5500]\n",
      "[9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500 9500\n",
      " 9500 9500 9500 9500 9500 9500]\n"
     ]
    }
   ],
   "source": [
    "print(len(test_trajectories))\n",
    "print(len(train_trajectories))\n",
    "\n",
    "print(dataset['omega_min'])\n",
    "print(dataset['omega_max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating mean and standard deviation for normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 53334.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n",
      "tensor([ 3.3701e-01, -9.1579e-02, -5.4900e-01,  3.5883e-01, -1.6437e-01,\n",
      "        -9.6327e-02,  8.0173e-03, -2.4643e-02,  2.0312e-02,  2.3068e-02,\n",
      "         6.6097e-02,  2.1455e-02,  7.5845e+03,  7.6346e+03,  7.7458e+03,\n",
      "         7.7316e+03])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 33826.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std:\n",
      "tensor([1.5155e+00, 1.5785e+00, 9.2298e-01, 1.3282e+00, 1.5455e+00, 5.1556e-01,\n",
      "        2.7391e-01, 2.5689e-01, 1.2481e+00, 1.2782e+00, 1.1084e+00, 1.1385e+00,\n",
      "        5.8533e+02, 6.2485e+02, 5.4435e+02, 6.2526e+02])\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "X_mean = torch.zeros(16)\n",
    "X_std = torch.zeros(16)\n",
    "\n",
    "N=10000\n",
    "\n",
    "for i, data in tqdm(enumerate(test_set)):\n",
    "    X = data[0]\n",
    "    X_mean += X\n",
    "    if i>=N:\n",
    "        break\n",
    "X_mean = X_mean/N\n",
    "\n",
    "print('mean:')\n",
    "print(X_mean)\n",
    "    \n",
    "for i, data in tqdm(enumerate(test_set)):\n",
    "    X = data[0]\n",
    "    X_std += (X-X_mean)**2\n",
    "    if i>=N:\n",
    "        break\n",
    "\n",
    "X_std = torch.sqrt(X_std/N)\n",
    "print('std:')\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aelarrassi/drone/normalize.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(mean, dtype=torch.float32)\n",
      "/home/aelarrassi/drone/normalize.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(std, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalize()\n",
       "  (1): Linear(in_features=16, out_features=120, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=120, out_features=120, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=120, out_features=120, bias=True)\n",
       "  (6): ReLU()\n",
       "  (7): Linear(in_features=120, out_features=4, bias=True)\n",
       "  (8): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomSigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.clamp((-1 / 2048) * x**2 + 0.25 * x + 32, 0.0, 1.0)\n",
    "    \n",
    "class PiecewiseSigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.where(x < -2.5, torch.zeros_like(x),\n",
    "               torch.where(x > 2.5, torch.ones_like(x),\n",
    "               0.2 * x + 0.5))\n",
    "class TanhSigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * (torch.tanh(x / 2) + 1)\n",
    "    \n",
    "\n",
    "class HardSigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(0.2 * x + 0.5, min=0.0, max=1.0)\n",
    "\n",
    "class FastSigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x / (1 + torch.abs(x))\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Normalize(mean=X_mean, std=X_std),\n",
    "    nn.Linear(16, 120),\n",
    "    # nn.BatchNorm1d(120, eps=1e-4, momentum=0.1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 120),\n",
    "    # nn.BatchNorm1d(120, eps=1e-4, momentum=0.1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 120),\n",
    "    # nn.BatchNorm1d(120, eps=1e-4, momentum=0.1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 4),\n",
    "    # nn.BatchNorm1d(4, eps=1e-4, momentum=0.1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6056, 0.4940, 0.4046, 0.5317]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x1 = torch.randn(1,16)\n",
    "print(model(x1))\n",
    "# print([param.shape for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a Loss function and optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=1,  threshold=0.001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Current learning rate: 0.0001, Time remaining: -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m start_time_epoch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     36\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (data, targets) \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[1;32m     39\u001b[0m     \n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/drone_env/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/drone_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/environments/drone_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/drone_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1453\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1453\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1455\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/environments/drone_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/drone_env/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:541\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 541\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    543\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:508\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthkey should be a byte string\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     \u001b[43manswer_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m     deliver_challenge(c, authkey)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:752\u001b[0m, in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(authkey, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthkey must be bytes, not \u001b[39m\u001b[38;5;132;01m{0!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(authkey)))\n\u001b[0;32m--> 752\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# reject large message\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m message[:\u001b[38;5;28mlen\u001b[39m(CHALLENGE)] \u001b[38;5;241m==\u001b[39m CHALLENGE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage = \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m message\n\u001b[1;32m    754\u001b[0m message \u001b[38;5;241m=\u001b[39m message[\u001b[38;5;28mlen\u001b[39m(CHALLENGE):]\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "loss_list = []\n",
    "loss_val_list = []\n",
    "best_loss = 0.1\n",
    "first = True\n",
    "start_time = time.time()\n",
    "\n",
    "# loop over the dataset multiple times\n",
    "num_epochs = 100\n",
    "\n",
    "nn_model_name = f\"{dataset_path[9:-4]}_{batchsize_train}_{batchsize_val}_{learning_rate}_{num_epochs}\"\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    if first:\n",
    "        time_remaining = '-'\n",
    "    else:\n",
    "        time_estimate = epoch_time*(num_epochs-epoch+1)\n",
    "        if time_estimate > 60:\n",
    "            if time_estimate > 3600:\n",
    "                time_remaining = str(round(time_estimate/3600,2))+' h'\n",
    "            else:\n",
    "                time_remaining = str(round(time_estimate/60,2))+' min'\n",
    "        else:\n",
    "            time_remaining = str(round(time_estimate,0))+' s'\n",
    "        \n",
    "    first = False\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Current learning rate: {optimizer.state_dict()['param_groups'][0]['lr']}, Time remaining: {time_remaining}\")\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    \n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    \n",
    "    for i, (data, targets) in loop:\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progressbar\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    # Validate\n",
    "    with torch.no_grad():\n",
    "        # Get a random batch from the test dataset\n",
    "        data_val, targets_val = next(iter(test_loader))\n",
    "\n",
    "        # Forward pass\n",
    "        outputs_val = model(data_val)\n",
    "\n",
    "        # Loss\n",
    "        loss_val = criterion(outputs_val, targets_val)\n",
    "\n",
    "        if loss_val < best_loss:\n",
    "            # Save best model\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "            # Backup\n",
    "            torch.save(model, 'neural_networks/tmp_benchmark.pt')\n",
    "            \n",
    "            best_loss = loss_val\n",
    "            print(\"Best model updated!\")\n",
    "\n",
    "        # Scheduler (reduce learning rate if loss stagnates)\n",
    "        scheduler.step(loss_val)\n",
    "        \n",
    "        loss_val_list.append(loss_val.item())\n",
    "\n",
    "    # print(\"Running mean:\", model[2].running_mean[:5])\n",
    "    # print(\"Running var:\", model[2].running_var[:5])\n",
    "    print(f'loss = {loss:.8f}, loss validation = {loss_val:.8f} '+r' (control error: +/-'+str(round(100*np.sqrt(float(loss_val)),2))+'%)\\n')\n",
    "\n",
    "    epoch_time = (time.time() - start_time_epoch)\n",
    "\n",
    "    loop.close()\n",
    "    \n",
    "# Compute excecution time\n",
    "execution_time = (time.time() - start_time)    \n",
    "print(f\"Total training time: {round(execution_time,2)}s\")\n",
    "\n",
    "# Save best model and copy for maptorange network\n",
    "torch.save(best_model, f'neural_networks/{nn_model_name}.pt')\n",
    "best_model_for_maptorange = torch.load('neural_networks/tmp_benchmark.pt', weights_only=False)\n",
    "print(best_model_for_maptorange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_val_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plotting loss curves\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mloss_val_list\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss (per batch)\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_val_list)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# plt.plot(running_mean(loss_list, N=100), label='Training Loss (smoothed)', color='blue')\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(loss_list)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# plt.plot(\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     np.linspace(0, len(loss_list), len(loss_val_list)), \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     loss_val_list, label='Validation Loss (per epoch)', color='orange'\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_val_list' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute running average (optional: smoother curve)\n",
    "def running_mean(x, N=100):\n",
    "    return np.convolve(x, np.ones(N)/N, mode='valid')\n",
    "\n",
    "# Plotting loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(loss_val_list, label='Validation Loss (per batch)', color='blue', alpha=0.3)\n",
    "print(loss_val_list)\n",
    "# plt.plot(running_mean(loss_list, N=100), label='Training Loss (smoothed)', color='blue')\n",
    "# print(loss_list)\n",
    "# plt.plot(\n",
    "#     np.linspace(0, len(loss_list), len(loss_val_list)), \n",
    "#     loss_val_list, label='Validation Loss (per epoch)', color='orange'\n",
    "# )\n",
    "\n",
    "plt.xlabel('Training Iterations (batches)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('training.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss = 0.0017092092020902783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loader = test_loader\n",
    "# loop over the test dataset\n",
    "loop = tqdm(enumerate(loader), total=len(loader), leave=False)\n",
    "running_loss = 0\n",
    "\n",
    "for i, (data, targets) in loop:\n",
    "    outputs = model(data)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    # update progressbar\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "\n",
    "loop.close()\n",
    "print('average loss =', running_loss/len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset_Generation.ipynb\t\t    quantized_weight_csvs\n",
      " Dataset_Generation_origin.ipynb\t    test_out.npz\n",
      " Drone.git\t\t\t\t    test_out_attempt_0.npz\n",
      "'Generate C code.ipynb'\t\t\t    test_out_attempt_1.npz\n",
      " HOVER_TO_HOVER_NOMINAL\t\t\t    test_out_attempt_10.npz\n",
      " HOVER_TO_HOVER_NOMINAL.npz\t\t    test_out_attempt_11.npz\n",
      "'Minimum Snap trajectories.ipynb'\t    test_out_attempt_12.npz\n",
      " Network_Training.ipynb\t\t\t    test_out_attempt_13.npz\n",
      " README.md\t\t\t\t    test_out_attempt_14.npz\n",
      " RedBit\t\t\t\t\t    test_out_attempt_15.npz\n",
      " Simulation.ipynb\t\t\t    test_out_attempt_16.npz\n",
      " __pycache__\t\t\t\t    test_out_attempt_17.npz\n",
      " ampl\t\t\t\t\t    test_out_attempt_18.npz\n",
      " c_code\t\t\t\t\t    test_out_attempt_19.npz\n",
      " datasets\t\t\t\t    test_out_attempt_2.npz\n",
      " layer_input_boxplots.png\t\t    test_out_attempt_3.npz\n",
      " neural_networks\t\t\t    test_out_attempt_4.npz\n",
      " normalize.py\t\t\t\t    test_out_attempt_5.npz\n",
      " pyquad\t\t\t\t\t    test_out_attempt_6.npz\n",
      " quadcopter_animation\t\t\t    test_out_attempt_7.npz\n",
      " quantized_weight_analysis.csv\t\t    test_out_attempt_8.npz\n",
      " quantized_weight_analysis_global.csv\t    test_out_attempt_9.npz\n",
      " quantized_weight_analysis_global_abs.csv   training.png\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Load your trained model (adjust path if needed)\n",
    "model = torch.load('neural_networks/HOVER_TO_HOVER_NOMINAL_new.pt', weights_only=False)\n",
    "model.eval()\n",
    "\n",
    "# Define target layers by their string names in model.named_modules()\n",
    "target_layers = ['1', '3', '5', '7']\n",
    "# target_layers = ['1', '4', '7', '10']\n",
    "\n",
    "# Dict to hold inputs for each target layer\n",
    "layer_inputs = {layer_name: [] for layer_name in target_layers}\n",
    "\n",
    "# Hook function to store input activations\n",
    "def hook_fn(layer_name):\n",
    "    def hook(module, input, output):\n",
    "        layer_inputs[layer_name].append(input[0].detach().cpu())\n",
    "    return hook\n",
    "\n",
    "# Register hooks for only the target layers\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if name in target_layers:\n",
    "        h = module.register_forward_hook(hook_fn(name))\n",
    "        hooks.append(h)\n",
    "\n",
    "# Run model on test_loader and collect activations\n",
    "# Run model on just the first 10 batches of test_loader\n",
    "with torch.no_grad():\n",
    "    for i, (data, targets) in enumerate(test_loader):\n",
    "        _ = model(data)\n",
    "        if i >= 9:  # stop after 10 batches (0-based index)\n",
    "            break\n",
    "\n",
    "# After inference, remove hooks (good practice)\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "# Prepare records for DataFrame: flatten all activations per layer\n",
    "records = []\n",
    "for layer_name in target_layers:\n",
    "    if len(layer_inputs[layer_name]) == 0:\n",
    "        print(f\"No activations collected for layer {layer_name}!\")\n",
    "        continue\n",
    "    # Concatenate batch tensors along dim=0 (batch dimension)\n",
    "    layer_tensor = torch.cat(layer_inputs[layer_name], dim=0)\n",
    "    # Flatten all activations\n",
    "    flat_activations = layer_tensor.numpy().flatten()\n",
    "    for val in flat_activations:\n",
    "        records.append({'Layer': f\"Layer {layer_name}\", 'Activation': val})\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Plot boxplot of activations per layer\n",
    "plt.figure(figsize=(10, len(target_layers)*1.5))\n",
    "sns.boxplot(data=df, y=\"Layer\", x=\"Activation\", orient=\"h\", linewidth=1, fliersize=1)\n",
    "plt.title(\"Input Activation Distributions (Linear Layers Only)\", fontsize=16)\n",
    "plt.xlabel(\"Activation Value\", fontsize=14)\n",
    "plt.ylabel(\"Layer\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"layer_input_summary_boxplot.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'neural_networks/HOVER_TO_HOVER_NOMINAL_BN.pt')\n",
    "torch.save({'network_state_dict': model.state_dict()}, 'neural_networks/Drone_customsigmoid_BN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('neural_networks/HOVER_TO_HOVER_NOMINAL_new.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': array([[0.        , 0.01133126, 0.02266252, ..., 2.22092738, 2.23225864,\n",
      "        2.24358991],\n",
      "       [0.        , 0.01171215, 0.02342429, ..., 2.29558065, 2.3072928 ,\n",
      "        2.31900495],\n",
      "       [0.        , 0.01244467, 0.02488935, ..., 2.43915621, 2.45160089,\n",
      "        2.46404556],\n",
      "       ...,\n",
      "       [0.        , 0.01160842, 0.02321685, ..., 2.27525095, 2.28685938,\n",
      "        2.2984678 ],\n",
      "       [0.        , 0.00981751, 0.01963503, ..., 1.92423264, 1.93405015,\n",
      "        1.94386767],\n",
      "       [0.        , 0.01068914, 0.02137828, ..., 2.09507182, 2.10576097,\n",
      "        2.11645011]]), 'dx': array([[-4.56538729e+00, -4.56794170e+00, -4.56652348e+00, ...,\n",
      "         1.60031017e-07, -3.67448446e-08,  0.00000000e+00],\n",
      "       [ 1.77525436e+00,  1.76413774e+00,  1.74862433e+00, ...,\n",
      "         2.04954080e-07, -4.11014763e-08,  0.00000000e+00],\n",
      "       [ 2.59538866e+00,  2.54422274e+00,  2.47515129e+00, ...,\n",
      "         5.85759222e-07, -1.25425660e-07,  0.00000000e+00],\n",
      "       ...,\n",
      "       [-9.45990141e-01, -9.21586949e-01, -8.97581504e-01, ...,\n",
      "         3.82716109e-07, -8.24857359e-08,  0.00000000e+00],\n",
      "       [-3.46481216e+00, -3.44895587e+00, -3.42892910e+00, ...,\n",
      "        -2.89027076e-07,  5.91239579e-08,  0.00000000e+00],\n",
      "       [-4.03793006e-01, -3.95822719e-01, -3.91391252e-01, ...,\n",
      "        -3.69260567e-07,  8.19759435e-08,  0.00000000e+00]]), 'dy': array([[ 4.57891802e-01,  4.91631240e-01,  5.25387962e-01, ...,\n",
      "         8.16791314e-07, -1.58683621e-07,  0.00000000e+00],\n",
      "       [-4.81394534e+00, -4.82617182e+00, -4.82826396e+00, ...,\n",
      "        -1.10110386e-06,  2.02749833e-07,  0.00000000e+00],\n",
      "       [ 4.61761300e+00,  4.65031268e+00,  4.67610238e+00, ...,\n",
      "         1.20711763e-06, -2.42465812e-07,  0.00000000e+00],\n",
      "       ...,\n",
      "       [-3.85550008e+00, -3.85658614e+00, -3.84956150e+00, ...,\n",
      "        -6.09598967e-07,  1.25873607e-07,  0.00000000e+00],\n",
      "       [ 7.75050058e-01,  7.83276831e-01,  7.87352985e-01, ...,\n",
      "         1.16358593e-07, -2.55254828e-08,  0.00000000e+00],\n",
      "       [-3.61510254e+00, -3.62013321e+00, -3.62397373e+00, ...,\n",
      "         3.00230212e-07, -7.08211266e-08,  0.00000000e+00]]), 'dz': array([[-5.02439756e-01, -4.87870120e-01, -5.04309731e-01, ...,\n",
      "         1.49412648e-05,  2.78932477e-06,  0.00000000e+00],\n",
      "       [-1.45773182e+00, -1.44404569e+00, -1.46696550e+00, ...,\n",
      "         6.11036658e-06,  1.13774951e-06,  0.00000000e+00],\n",
      "       [-3.06225282e+00, -3.05415383e+00, -3.06886441e+00, ...,\n",
      "        -8.57109961e-05, -1.60153675e-05,  0.00000000e+00],\n",
      "       ...,\n",
      "       [ 1.37869958e+00,  1.40604655e+00,  1.45426035e+00, ...,\n",
      "         2.67272212e-05,  4.99160049e-06,  0.00000000e+00],\n",
      "       [-1.43838198e+00, -1.47143899e+00, -1.51377505e+00, ...,\n",
      "         5.09824205e-06,  9.52606227e-07,  0.00000000e+00],\n",
      "       [-6.34504012e-02, -7.10007910e-02, -1.10169001e-01, ...,\n",
      "        -3.29166518e-06, -6.16155450e-07,  0.00000000e+00]]), 'vx': array([[ 3.57884489e-01,  3.26736870e-01,  2.94710233e-01, ...,\n",
      "         4.27835757e-05,  3.57909986e-07,  0.00000000e+00],\n",
      "       [-3.39384656e-01, -2.65945001e-01, -1.93102281e-01, ...,\n",
      "         4.97244506e-05,  8.94833581e-07,  0.00000000e+00],\n",
      "       [-2.11350194e-01, -2.24773353e-01, -2.35861797e-01, ...,\n",
      "         1.17268089e-04,  8.34852836e-07,  0.00000000e+00],\n",
      "       ...,\n",
      "       [ 4.27301987e-01,  3.50865327e-01,  2.73616164e-01, ...,\n",
      "         1.00376291e-04,  1.42855516e-06,  0.00000000e+00],\n",
      "       [-4.00204725e-02, -7.85618046e-02, -1.17524912e-01, ...,\n",
      "        -8.42581865e-05, -1.40147302e-06,  0.00000000e+00],\n",
      "       [ 3.59350238e-01,  3.44442421e-01,  3.30974065e-01, ...,\n",
      "        -9.89196913e-05, -9.27630192e-07,  0.00000000e+00]]), 'vy': array([[-3.64421766e-01, -3.81357330e-01, -3.97713473e-01, ...,\n",
      "         2.07067833e-04,  4.36731198e-06,  0.00000000e+00],\n",
      "       [ 1.53086996e-01,  1.41836001e-01,  1.28013267e-01, ...,\n",
      "        -2.60485492e-04, -6.35884485e-06,  0.00000000e+00],\n",
      "       [ 2.57827277e-01,  3.11635689e-01,  3.64454720e-01, ...,\n",
      "         2.37741932e-04,  3.04699459e-06,  0.00000000e+00],\n",
      "       ...,\n",
      "       [ 3.12814804e-01,  3.34810068e-01,  3.55476328e-01, ...,\n",
      "        -1.57629024e-04, -2.74692237e-06,  0.00000000e+00],\n",
      "       [ 2.90339290e-01,  3.27029925e-01,  3.64871006e-01, ...,\n",
      "         3.46304423e-05,  3.89812016e-07,  0.00000000e+00],\n",
      "       [ 3.59127349e-01,  3.75123494e-01,  3.93782024e-01, ...,\n",
      "         8.19921795e-05,  3.61645308e-07,  0.00000000e+00]]), 'vz': array([[ 1.10732432e-01,  1.11000071e-01,  1.12787431e-01, ...,\n",
      "         1.65213504e-03,  5.75829718e-04,  0.00000000e+00],\n",
      "       [ 2.90709722e-01,  2.53422111e-01,  2.18931386e-01, ...,\n",
      "         6.54250907e-04,  2.27601488e-04,  0.00000000e+00],\n",
      "       [ 1.24003905e-01,  1.19704653e-01,  1.13431295e-01, ...,\n",
      "        -8.62762425e-03, -3.00873350e-03,  0.00000000e+00],\n",
      "       ...,\n",
      "       [-3.53780089e-02, -6.11741065e-02, -8.81325629e-02, ...,\n",
      "         2.88450864e-03,  1.00561457e-03,  0.00000000e+00],\n",
      "       [ 3.08411800e-01,  2.61472770e-01,  2.15622104e-01, ...,\n",
      "         6.50391144e-04,  2.26861184e-04,  0.00000000e+00],\n",
      "       [-4.31403373e-01, -4.53328413e-01, -4.67428383e-01, ...,\n",
      "        -3.85474559e-04, -1.34620047e-04,  0.00000000e+00]]), 'phi': array([[-1.48486148e-01, -1.47155502e-01, -1.44759733e-01, ...,\n",
      "        -3.63976909e-03, -5.12225066e-04,  0.00000000e+00],\n",
      "       [-1.42313179e-01, -1.43994716e-01, -1.53907664e-01, ...,\n",
      "         4.34016993e-03,  6.25328448e-04,  0.00000000e+00],\n",
      "       [ 4.90938812e-01,  4.79453141e-01,  4.69911411e-01, ...,\n",
      "        -3.43678277e-03, -4.78744642e-04,  0.00000000e+00],\n",
      "       ...,\n",
      "       [ 2.17315669e-01,  2.19739113e-01,  2.27555287e-01, ...,\n",
      "         2.76841358e-03,  3.79547179e-04,  0.00000000e+00],\n",
      "       [ 3.99667923e-01,  4.12500626e-01,  4.27197800e-01, ...,\n",
      "        -7.08562774e-04, -9.45677482e-05,  0.00000000e+00],\n",
      "       [ 1.45043665e-01,  1.41979740e-01,  1.30767753e-01, ...,\n",
      "        -1.53976480e-03, -1.98170000e-04,  0.00000000e+00]]), 'theta': array([[ 2.55823767e-01,  2.53387800e-01,  2.57528847e-01, ...,\n",
      "         7.73924758e-04,  1.00961789e-04,  0.00000000e+00],\n",
      "       [-6.92415058e-01, -6.86346688e-01, -6.80039222e-01, ...,\n",
      "         8.40981640e-04,  1.16608149e-04,  0.00000000e+00],\n",
      "       [ 1.33994085e-01,  1.21540305e-01,  1.03996796e-01, ...,\n",
      "         1.71469964e-03,  2.30705960e-04,  0.00000000e+00],\n",
      "       ...,\n",
      "       [ 6.76015974e-01,  6.89085423e-01,  7.02833185e-01, ...,\n",
      "         1.77605597e-03,  2.38889835e-04,  0.00000000e+00],\n",
      "       [ 4.15228766e-01,  4.18691426e-01,  4.24464233e-01, ...,\n",
      "        -1.70560030e-03, -2.34779891e-04,  0.00000000e+00],\n",
      "       [ 1.41361455e-01,  1.47713369e-01,  1.57629146e-01, ...,\n",
      "        -1.83768574e-03, -2.43749132e-04,  0.00000000e+00]]), 'psi': array([[-1.84687973e+00, -1.83980546e+00, -1.83362641e+00, ...,\n",
      "        -4.94040565e-04, -5.72832402e-05,  0.00000000e+00],\n",
      "       [ 7.78084376e-02,  8.28083968e-02,  8.86892897e-02, ...,\n",
      "         3.71236479e-04,  4.27255413e-05,  0.00000000e+00],\n",
      "       [-2.66204454e-01, -2.74799363e-01, -2.85968110e-01, ...,\n",
      "        -7.05501469e-04, -8.08921451e-05,  0.00000000e+00],\n",
      "       ...,\n",
      "       [ 7.24819078e-01,  7.13485426e-01,  7.02275438e-01, ...,\n",
      "         4.19116559e-04,  4.81587909e-05,  0.00000000e+00],\n",
      "       [-5.13025881e-02, -4.13767030e-02, -3.05936238e-02, ...,\n",
      "         2.87705915e-04,  3.37486356e-05,  0.00000000e+00],\n",
      "       [-2.04774722e+00, -2.05001842e+00, -2.05065989e+00, ...,\n",
      "        -3.73525534e-04, -4.41989326e-05,  0.00000000e+00]]), 'p': array([[-0.05291778, -0.00449436,  0.17421447, ...,  0.4615594 ,\n",
      "         0.12550668,  0.        ],\n",
      "       [ 0.46097194, -0.20296404, -0.85358251, ..., -0.52753315,\n",
      "        -0.1460326 ,  0.        ],\n",
      "       [-0.90674621, -0.75658532, -0.56839744, ...,  0.39830068,\n",
      "         0.10750631,  0.        ],\n",
      "       ...,\n",
      "       [ 0.45122029,  1.12813204,  1.38705554, ..., -0.3460842 ,\n",
      "        -0.09231349,  0.        ],\n",
      "       [ 0.73024878,  1.0191168 ,  1.02999582, ...,  0.10573995,\n",
      "         0.02767336,  0.        ],\n",
      "       [ 0.09979683, -0.63008079, -1.46774298, ...,  0.2140394 ,\n",
      "         0.05455487,  0.        ]]), 'q': array([[-0.62544175,  0.0068119 ,  0.54746696, ..., -0.10120295,\n",
      "        -0.02599318,  0.        ],\n",
      "       [ 0.50281401,  0.4500238 ,  0.52059357, ..., -0.1039946 ,\n",
      "        -0.02791446,  0.        ],\n",
      "       [-0.94594663, -1.4482316 , -1.85940525, ..., -0.20171664,\n",
      "        -0.05299487,  0.        ],\n",
      "       ...,\n",
      "       [ 0.87397148,  0.97981848,  0.9851784 , ..., -0.22383143,\n",
      "        -0.0588333 ,  0.        ],\n",
      "       [ 0.54791916,  0.82369002,  1.06054755, ...,  0.2518349 ,\n",
      "         0.06734801,  0.        ],\n",
      "       [ 0.32662866,  0.76176017,  1.03209707, ...,  0.25254541,\n",
      "         0.06578108,  0.        ]]), 'r': array([[ 0.56376943,  0.56934693,  0.58206882, ...,  0.06661032,\n",
      "         0.01594196,  0.        ],\n",
      "       [ 0.35865921,  0.43642748,  0.48916475, ..., -0.04835063,\n",
      "        -0.01155469,  0.        ],\n",
      "       [-0.13590555, -0.14452625, -0.15400821, ...,  0.08668863,\n",
      "         0.02064753,  0.        ],\n",
      "       ...,\n",
      "       [-0.98444949, -0.9831534 , -0.98740955, ..., -0.05499508,\n",
      "        -0.01315241,  0.        ],\n",
      "       [ 0.72991232,  0.69113991,  0.66107867, ..., -0.04468188,\n",
      "        -0.01075757,  0.        ],\n",
      "       [-0.351024  , -0.23780173, -0.13396834, ...,  0.05373786,\n",
      "         0.01288407,  0.        ]]), 'omega': array([[[7651.80755554, 8671.16343536, 5652.58192609, 6653.21554925],\n",
      "        [7731.73919803, 8409.16039543, 5719.26524702, 6963.02908138],\n",
      "        [7763.05244235, 8236.15215927, 5845.69713585, 7236.70706036],\n",
      "        ...,\n",
      "        [7620.55815161, 7730.81544649, 8097.88194946, 6589.36507329],\n",
      "        [7723.73515887, 7416.1079585 , 8010.38167347, 6772.51365913],\n",
      "        [7866.71366363, 7108.95096679, 7866.71366363, 7108.95096679]],\n",
      "\n",
      "       [[8000.59956633, 7060.26524557, 9244.27240217, 6006.05191155],\n",
      "        [7978.70296908, 7430.19467621, 8900.40735104, 5949.25564629],\n",
      "        [8002.01642171, 7678.02142683, 8633.86905174, 5966.75918197],\n",
      "        ...,\n",
      "        [7882.26485018, 7406.80872155, 6711.24331224, 7973.33646693],\n",
      "        [7596.78399005, 7536.32038029, 6909.99265027, 7877.77418716],\n",
      "        [7269.53778004, 7718.56127645, 7269.53778004, 7718.56127645]],\n",
      "\n",
      "       [[6583.65144984, 6556.80935424, 7797.92173841, 8266.48978617],\n",
      "        [6919.88655977, 6757.82503969, 7751.54680504, 8287.24328181],\n",
      "        [7246.53024795, 6996.96480772, 7748.76412935, 8305.59240874],\n",
      "        ...,\n",
      "        [7596.09101644, 7604.77420076, 7701.69120226, 6366.93741127],\n",
      "        [7746.3930172 , 7352.22361247, 7816.81551685, 6668.80992386],\n",
      "        [7868.80117854, 7106.64025587, 7868.80117854, 7106.64025587]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[9001.24420859, 5964.52536941, 7067.34793555, 7527.40951466],\n",
      "        [8385.1925636 , 6288.63881335, 7495.37842016, 7187.28621976],\n",
      "        [7877.81766917, 6519.16655614, 7847.90068984, 6939.50344342],\n",
      "        ...,\n",
      "        [7936.49916628, 7775.64757954, 6687.95525058, 7749.65830855],\n",
      "        [7578.56380529, 7755.79566422, 6887.84253653, 7748.92869613],\n",
      "        [7235.99975576, 7750.01130634, 7235.99975576, 7750.01130634]],\n",
      "\n",
      "       [[8597.58762718, 9126.55880425, 6537.68744848, 9160.24284454],\n",
      "        [8186.72991843, 9179.57305269, 6934.76063361, 8736.29471367],\n",
      "        [7899.60147046, 9218.00187449, 7266.71640788, 8451.11564326],\n",
      "        ...,\n",
      "        [6736.87552669, 7436.76269579, 7801.67989598, 8012.02042067],\n",
      "        [6932.14745323, 7575.67062488, 7523.37861731, 7899.39255109],\n",
      "        [7240.58070273, 7745.73165046, 7240.58070273, 7745.73165046]],\n",
      "\n",
      "       [[9418.49423338, 8676.68790781, 9179.1925017 , 5783.38590031],\n",
      "        [8784.56761348, 8623.25409895, 9230.38029722, 6061.32983464],\n",
      "        [8265.34253687, 8545.76581222, 9270.90544219, 6365.40424176],\n",
      "        ...,\n",
      "        [7369.26190838, 6804.77168561, 8495.05246516, 7181.58348159],\n",
      "        [7671.03107886, 6885.5884171 , 8261.20598408, 7094.18462927],\n",
      "        [7954.45601656, 7010.63457858, 7954.45601656, 7010.63457858]]]), 'u': array([[[6.79994460e-01, 3.79638127e-01, 8.18931821e-02, 7.25355061e-01],\n",
      "        [6.29540429e-01, 4.43070487e-01, 1.85126025e-01, 7.50456290e-01],\n",
      "        [5.79086398e-01, 5.06502848e-01, 2.88358867e-01, 7.75557520e-01],\n",
      "        ...,\n",
      "        [6.37061470e-01, 1.35476337e-01, 5.75497393e-01, 4.00554448e-01],\n",
      "        [7.20518991e-01, 6.77384080e-02, 4.72248121e-01, 6.68421900e-01],\n",
      "        [8.03976511e-01, 4.78545800e-07, 3.68998850e-01, 9.36289353e-01]],\n",
      "\n",
      "       [[5.64388193e-01, 9.52207968e-01, 4.39711740e-01, 2.43300785e-06],\n",
      "        [6.22466793e-01, 8.73047936e-01, 4.62445372e-01, 9.02482105e-02],\n",
      "        [6.80545393e-01, 7.93887905e-01, 4.85179004e-01, 1.80493988e-01],\n",
      "        ...,\n",
      "        [2.60170016e-01, 6.04410732e-01, 4.40986441e-01, 5.42009022e-01],\n",
      "        [1.30089342e-01, 7.10911281e-01, 7.16707912e-01, 4.28643377e-01],\n",
      "        [8.66710165e-06, 8.17411829e-01, 9.92429383e-01, 3.15277733e-01]],\n",
      "\n",
      "       [[6.82768483e-01, 4.80340401e-01, 4.88678789e-01, 7.18286757e-01],\n",
      "        [7.54067457e-01, 5.81312118e-01, 5.35077356e-01, 7.20276515e-01],\n",
      "        [8.25366432e-01, 6.82283835e-01, 5.81475922e-01, 7.22266274e-01],\n",
      "        ...,\n",
      "        [7.24322189e-01, 2.17005997e-01, 7.32499314e-01, 4.87324646e-01],\n",
      "        [7.24789543e-01, 1.63137101e-01, 6.77284842e-01, 7.43662201e-01],\n",
      "        [7.25256896e-01, 1.09268205e-01, 6.22070370e-01, 9.99999755e-01]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[3.18609955e-08, 6.03202340e-01, 9.99999955e-01, 2.26516903e-06],\n",
      "        [2.46556833e-08, 5.51604307e-01, 9.99999965e-01, 4.58332067e-02],\n",
      "        [1.74503711e-08, 5.00006273e-01, 9.99999975e-01, 9.16641483e-02],\n",
      "        ...,\n",
      "        [1.35401111e-01, 5.32998841e-01, 4.47125750e-01, 5.60149933e-01],\n",
      "        [6.77009580e-02, 5.47971915e-01, 7.07220554e-01, 5.62535749e-01],\n",
      "        [8.04717420e-07, 5.62944988e-01, 9.67315359e-01, 5.64921565e-01]],\n",
      "\n",
      "       [[4.18222840e-02, 9.99997062e-01, 9.21275053e-01, 1.49742636e-01],\n",
      "        [1.43617698e-01, 9.89142638e-01, 9.12911967e-01, 2.73124456e-01],\n",
      "        [2.45413112e-01, 9.78288214e-01, 9.04548881e-01, 3.96506275e-01],\n",
      "        ...,\n",
      "        [5.11692629e-01, 6.70030353e-01, 1.54018403e-01, 4.90689264e-01],\n",
      "        [7.47552900e-01, 7.56249721e-01, 7.70106762e-02, 3.94709547e-01],\n",
      "        [9.83413171e-01, 8.42469089e-01, 2.94894495e-06, 2.98729830e-01]],\n",
      "\n",
      "       [[2.48311883e-07, 7.38070960e-01, 9.99999576e-01, 4.40371406e-01],\n",
      "        [1.68161795e-02, 6.87950213e-01, 9.96500869e-01, 5.49792413e-01],\n",
      "        [3.36321107e-02, 6.37829466e-01, 9.93002162e-01, 6.59213419e-01],\n",
      "        ...,\n",
      "        [9.05185950e-01, 4.04883275e-01, 4.77835718e-01, 2.94728425e-01],\n",
      "        [9.52592877e-01, 4.92682994e-01, 3.07956196e-01, 2.78760809e-01],\n",
      "        [9.99999804e-01, 5.80482713e-01, 1.38076675e-01, 2.62793194e-01]]]), 'omega_min': array([5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500,\n",
      "       5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500, 5500]), 'omega_max': array([9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500,\n",
      "       9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500, 9500]), 'k_omega': array(4.36301076e-08), 'Mx_ext': array([ 0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,\n",
      "       -0., -0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.,  0.,\n",
      "       -0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0.,\n",
      "        0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0., -0., -0.,\n",
      "        0., -0.,  0.,  0.,  0., -0., -0., -0.,  0., -0., -0., -0.,  0.,\n",
      "        0.,  0., -0., -0., -0.,  0.,  0., -0., -0., -0., -0., -0., -0.,\n",
      "       -0., -0.,  0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,  0., -0.,\n",
      "       -0.,  0., -0.,  0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
      "        0., -0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0., -0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,\n",
      "        0., -0., -0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0., -0.,  0.,\n",
      "       -0., -0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0.,\n",
      "       -0., -0., -0.,  0., -0., -0.,  0., -0., -0.,  0.,  0.,  0., -0.,\n",
      "       -0.,  0.,  0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0., -0.,\n",
      "        0.,  0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0.,  0.,\n",
      "       -0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.,  0., -0.,\n",
      "        0., -0., -0.,  0., -0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0.,\n",
      "       -0.,  0., -0., -0., -0., -0., -0.,  0.,  0., -0.,  0., -0., -0.,\n",
      "        0.,  0., -0., -0., -0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,\n",
      "       -0., -0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,  0.,\n",
      "        0.,  0., -0., -0.,  0., -0.,  0., -0., -0.,  0.,  0.,  0., -0.,\n",
      "        0., -0., -0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,\n",
      "        0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,  0.,  0.,  0.,\n",
      "       -0., -0., -0., -0.,  0.,  0., -0.,  0., -0., -0., -0., -0., -0.,\n",
      "       -0., -0., -0., -0.,  0., -0., -0., -0., -0., -0.,  0.,  0.,  0.,\n",
      "        0., -0., -0., -0.,  0., -0.,  0., -0., -0., -0.,  0., -0., -0.,\n",
      "        0., -0.,  0.,  0., -0.,  0., -0., -0.,  0., -0.,  0.,  0., -0.,\n",
      "        0., -0.,  0., -0.,  0., -0., -0.,  0.,  0.,  0., -0., -0., -0.,\n",
      "        0.,  0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,  0.,  0., -0.,\n",
      "       -0., -0., -0., -0., -0., -0.,  0., -0., -0.,  0., -0.,  0.,  0.,\n",
      "       -0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,  0., -0., -0., -0.,\n",
      "       -0., -0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0.,  0.,\n",
      "        0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0., -0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,\n",
      "       -0.,  0., -0., -0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0.,\n",
      "        0., -0.,  0., -0.,  0., -0., -0.,  0., -0., -0.,  0., -0., -0.,\n",
      "        0., -0., -0., -0., -0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0.,\n",
      "        0., -0., -0., -0.,  0.,  0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
      "       -0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0.,\n",
      "        0., -0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0.,  0., -0.,\n",
      "       -0., -0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0., -0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,\n",
      "       -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,  0.,  0.,\n",
      "       -0., -0.,  0., -0., -0.,  0.,  0.,  0., -0.,  0., -0., -0., -0.,\n",
      "       -0., -0., -0., -0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.,\n",
      "        0.,  0., -0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0., -0.,\n",
      "        0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0.,  0., -0.,  0., -0., -0.,  0., -0.,  0.,  0.,\n",
      "        0.,  0.,  0., -0., -0.,  0., -0., -0.,  0., -0.,  0., -0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0.,\n",
      "       -0., -0.,  0., -0., -0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0.,\n",
      "       -0., -0.,  0.,  0.,  0.,  0., -0., -0., -0., -0., -0.,  0., -0.,\n",
      "        0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0., -0., -0.,\n",
      "       -0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,  0., -0.,\n",
      "        0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0., -0.,  0.,\n",
      "        0.,  0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,  0.,\n",
      "       -0.,  0., -0., -0., -0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,\n",
      "       -0., -0.,  0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0.,\n",
      "        0.,  0., -0., -0.,  0.,  0.,  0., -0., -0., -0., -0., -0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0.,  0., -0.,  0., -0., -0.,  0., -0.,  0., -0.,\n",
      "       -0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0., -0., -0., -0., -0.,\n",
      "        0.,  0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0.,\n",
      "       -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0., -0., -0.,\n",
      "       -0., -0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,\n",
      "       -0.,  0.,  0.,  0.,  0., -0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0.,\n",
      "        0., -0.,  0.,  0., -0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,\n",
      "        0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0., -0., -0., -0.,  0.,\n",
      "        0.,  0., -0., -0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0.,  0.,\n",
      "       -0., -0., -0., -0.,  0., -0., -0.,  0.,  0.,  0.,  0.,  0., -0.,\n",
      "       -0., -0., -0., -0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.,\n",
      "       -0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,\n",
      "        0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0.]), 'My_ext': array([-0., -0.,  0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,  0., -0.,\n",
      "       -0.,  0.,  0., -0.,  0., -0.,  0., -0., -0.,  0., -0., -0.,  0.,\n",
      "        0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0.,  0.,\n",
      "       -0.,  0., -0.,  0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0.,\n",
      "       -0., -0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0.,\n",
      "        0., -0.,  0., -0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,\n",
      "       -0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.,\n",
      "       -0.,  0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0.,\n",
      "       -0.,  0.,  0.,  0.,  0., -0., -0., -0., -0., -0.,  0.,  0., -0.,\n",
      "        0.,  0.,  0., -0., -0., -0.,  0., -0.,  0.,  0., -0., -0.,  0.,\n",
      "       -0.,  0.,  0.,  0.,  0., -0., -0., -0., -0.,  0.,  0., -0., -0.,\n",
      "       -0., -0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0., -0., -0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0., -0.,\n",
      "        0., -0., -0., -0., -0.,  0.,  0., -0., -0., -0.,  0., -0.,  0.,\n",
      "        0., -0.,  0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0.,\n",
      "        0.,  0., -0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0., -0.,\n",
      "        0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,\n",
      "       -0.,  0., -0., -0., -0.,  0., -0.,  0., -0., -0., -0.,  0.,  0.,\n",
      "        0., -0., -0., -0., -0.,  0., -0.,  0.,  0., -0., -0., -0., -0.,\n",
      "       -0.,  0., -0., -0.,  0., -0., -0.,  0., -0., -0., -0.,  0.,  0.,\n",
      "        0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.,  0., -0., -0.,\n",
      "       -0., -0., -0.,  0., -0.,  0.,  0.,  0., -0.,  0., -0., -0., -0.,\n",
      "       -0.,  0., -0., -0., -0.,  0., -0.,  0.,  0., -0., -0.,  0., -0.,\n",
      "        0., -0.,  0., -0., -0., -0., -0.,  0.,  0.,  0., -0., -0.,  0.,\n",
      "        0., -0.,  0.,  0., -0., -0., -0., -0., -0., -0.,  0.,  0.,  0.,\n",
      "       -0., -0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0.,\n",
      "        0., -0., -0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0., -0.,  0.,\n",
      "       -0., -0., -0., -0.,  0., -0.,  0.,  0., -0., -0., -0., -0., -0.,\n",
      "        0., -0., -0.,  0.,  0., -0.,  0., -0., -0.,  0., -0., -0.,  0.,\n",
      "       -0.,  0.,  0.,  0.,  0., -0.,  0., -0., -0., -0., -0.,  0., -0.,\n",
      "        0., -0., -0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,  0., -0.,\n",
      "        0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0.,  0.,  0.,  0.,\n",
      "       -0., -0.,  0., -0., -0., -0., -0.,  0., -0., -0.,  0., -0.,  0.,\n",
      "       -0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0., -0., -0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0., -0.,  0., -0., -0., -0., -0., -0.,  0., -0.,\n",
      "        0., -0.,  0., -0., -0., -0.,  0.,  0.,  0.,  0., -0.,  0., -0.,\n",
      "       -0., -0., -0.,  0., -0.,  0.,  0., -0., -0.,  0., -0., -0., -0.,\n",
      "       -0.,  0., -0.,  0., -0., -0., -0., -0., -0.,  0., -0., -0.,  0.,\n",
      "       -0., -0., -0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,  0.,\n",
      "       -0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,\n",
      "        0., -0.,  0.,  0., -0., -0., -0., -0.,  0., -0.,  0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,\n",
      "       -0., -0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.,\n",
      "       -0.,  0.,  0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,\n",
      "        0., -0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,  0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0., -0.,  0., -0., -0., -0., -0.,  0.,  0.,  0.,\n",
      "       -0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0., -0.,  0.,  0., -0.,\n",
      "       -0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,\n",
      "       -0., -0., -0., -0., -0.,  0.,  0.,  0., -0., -0., -0., -0.,  0.,\n",
      "        0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,\n",
      "        0.,  0.,  0.,  0., -0., -0., -0., -0., -0.,  0.,  0., -0.,  0.,\n",
      "       -0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0.,\n",
      "       -0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,\n",
      "       -0.,  0.,  0.,  0., -0.,  0., -0., -0., -0.,  0., -0., -0.,  0.,\n",
      "       -0., -0., -0., -0.,  0., -0., -0.,  0.,  0.,  0., -0., -0.,  0.,\n",
      "       -0.,  0., -0., -0., -0.,  0., -0., -0., -0.,  0., -0.,  0., -0.,\n",
      "        0., -0.,  0., -0., -0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,\n",
      "        0., -0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0.,\n",
      "       -0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0., -0.,\n",
      "        0.,  0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0., -0.,  0., -0.,\n",
      "        0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0., -0.,  0.,\n",
      "       -0., -0.,  0., -0.,  0.,  0., -0., -0.,  0.,  0., -0.,  0., -0.,\n",
      "        0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,\n",
      "       -0., -0., -0., -0.,  0.,  0.,  0., -0., -0., -0.,  0., -0.,  0.,\n",
      "       -0.,  0., -0., -0.,  0.,  0.,  0., -0.,  0., -0., -0., -0., -0.,\n",
      "       -0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0., -0., -0., -0.,  0.,\n",
      "       -0.,  0.,  0., -0.,  0., -0., -0., -0., -0.,  0., -0.,  0., -0.,\n",
      "        0.,  0.,  0.,  0., -0., -0., -0., -0.,  0.,  0., -0.,  0., -0.,\n",
      "       -0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0., -0.,  0.,\n",
      "        0., -0., -0.,  0.,  0., -0.,  0., -0., -0., -0.,  0.,  0., -0.,\n",
      "        0., -0., -0.,  0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0., -0.,\n",
      "        0., -0.,  0., -0., -0., -0., -0.,  0.,  0., -0., -0., -0.,  0.,\n",
      "        0., -0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0.,  0., -0.,\n",
      "        0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0.,\n",
      "        0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0.,\n",
      "        0., -0., -0., -0.,  0., -0.,  0., -0.,  0.,  0., -0., -0.]), 'Mz_ext': array([-0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0.,  0.,\n",
      "       -0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0.,\n",
      "        0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0.,\n",
      "       -0., -0.,  0.,  0., -0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,\n",
      "       -0., -0., -0.,  0., -0., -0., -0.,  0., -0., -0.,  0., -0., -0.,\n",
      "        0., -0., -0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0.,  0.,\n",
      "        0.,  0., -0., -0.,  0.,  0., -0., -0., -0., -0., -0., -0.,  0.,\n",
      "        0.,  0.,  0.,  0., -0., -0., -0.,  0., -0.,  0.,  0., -0.,  0.,\n",
      "        0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0., -0.,  0., -0.,  0.,\n",
      "        0., -0.,  0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0.,  0., -0.,\n",
      "        0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.,\n",
      "       -0.,  0., -0., -0., -0., -0., -0.,  0., -0.,  0.,  0., -0., -0.,\n",
      "       -0.,  0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0., -0., -0.,\n",
      "       -0.,  0., -0., -0.,  0.,  0.,  0., -0.,  0., -0., -0.,  0.,  0.,\n",
      "        0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0., -0.,\n",
      "        0., -0., -0., -0., -0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,\n",
      "        0.,  0.,  0.,  0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,\n",
      "        0.,  0., -0., -0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.,\n",
      "       -0., -0., -0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,  0., -0.,\n",
      "       -0., -0., -0., -0., -0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0., -0., -0., -0., -0., -0.,  0.,  0.,  0.,\n",
      "        0., -0.,  0., -0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0.,\n",
      "       -0.,  0., -0., -0., -0., -0.,  0., -0., -0.,  0., -0.,  0.,  0.,\n",
      "       -0.,  0.,  0.,  0., -0.,  0., -0., -0.,  0.,  0.,  0., -0., -0.,\n",
      "       -0., -0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0., -0.,  0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0.,\n",
      "       -0., -0., -0.,  0., -0., -0., -0., -0.,  0., -0.,  0.,  0.,  0.,\n",
      "        0., -0., -0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,\n",
      "       -0., -0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0., -0.,  0.,\n",
      "        0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,\n",
      "       -0., -0., -0.,  0., -0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0.,\n",
      "        0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,\n",
      "        0., -0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,  0., -0., -0.,\n",
      "        0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0.,\n",
      "       -0., -0.,  0., -0.,  0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.,\n",
      "       -0., -0.,  0., -0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0., -0.,\n",
      "       -0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0.,\n",
      "       -0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,\n",
      "        0.,  0., -0., -0.,  0., -0., -0., -0.,  0., -0., -0.,  0., -0.,\n",
      "       -0., -0., -0.,  0., -0.,  0., -0.,  0., -0., -0.,  0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0.,  0.,\n",
      "        0.,  0.,  0.,  0., -0., -0.,  0., -0., -0., -0.,  0.,  0., -0.,\n",
      "       -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,  0., -0., -0.,\n",
      "       -0., -0., -0.,  0.,  0., -0.,  0.,  0., -0., -0., -0., -0., -0.,\n",
      "        0., -0., -0.,  0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,  0.,\n",
      "       -0., -0., -0., -0., -0.,  0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
      "        0.,  0., -0.,  0., -0., -0., -0.,  0., -0., -0.,  0., -0., -0.,\n",
      "       -0.,  0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0.,  0.,\n",
      "       -0., -0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "       -0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0.,\n",
      "        0., -0.,  0.,  0.,  0., -0.,  0., -0., -0.,  0., -0., -0.,  0.,\n",
      "        0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0.,\n",
      "        0., -0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,\n",
      "        0.,  0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0., -0.,  0.,\n",
      "        0., -0.,  0., -0., -0.,  0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
      "       -0.,  0.,  0.,  0., -0., -0.,  0., -0., -0., -0., -0., -0., -0.,\n",
      "        0.,  0.,  0., -0., -0.,  0., -0., -0., -0., -0.,  0., -0., -0.,\n",
      "       -0., -0., -0.,  0., -0., -0.,  0.,  0., -0., -0.,  0., -0., -0.,\n",
      "        0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0., -0.,  0.,  0., -0.,\n",
      "        0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0.,  0.,\n",
      "        0.,  0., -0., -0., -0., -0., -0.,  0., -0.,  0., -0., -0., -0.,\n",
      "       -0.,  0., -0., -0., -0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,\n",
      "       -0., -0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,\n",
      "       -0., -0.,  0., -0., -0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,\n",
      "       -0., -0., -0., -0.,  0., -0.,  0.,  0., -0., -0., -0.,  0.,  0.,\n",
      "       -0., -0.,  0., -0., -0., -0., -0., -0., -0.,  0.,  0., -0.,  0.,\n",
      "        0.,  0., -0., -0., -0.,  0., -0.,  0.,  0.,  0.,  0., -0.,  0.,\n",
      "       -0., -0., -0., -0.,  0., -0.,  0., -0., -0., -0., -0., -0.,  0.,\n",
      "       -0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0., -0., -0.,\n",
      "       -0.,  0., -0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0., -0.,  0.,\n",
      "        0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,\n",
      "       -0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.,  0.,\n",
      "       -0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0., -0., -0.,  0., -0.,\n",
      "        0., -0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0., -0., -0.,  0.,\n",
      "       -0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0., -0.,  0.,  0.,\n",
      "        0.,  0.,  0., -0., -0., -0.,  0.,  0., -0., -0., -0.,  0.,  0.,\n",
      "        0., -0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0., -0.])}\n"
     ]
    }
   ],
   "source": [
    "model_processed_output =nn.Sequential(\n",
    "    *model,\n",
    "    MapToRange(dataset['omega_min'], dataset['omega_max'])\n",
    ")\n",
    "\n",
    "print(dataset)\n",
    "torch.save(model_processed_output, 'neural_networks/HOVER_TO_HOVER_NOMINAL_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize()\n",
      "Sequential(\n",
      "  (0): Normalize()\n",
      "  (1): DynamicQuantizedLinear(in_features=16, out_features=120, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (2): BatchNorm1d(120, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): ReLU()\n",
      "  (4): DynamicQuantizedLinear(in_features=120, out_features=120, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (5): BatchNorm1d(120, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU()\n",
      "  (7): DynamicQuantizedLinear(in_features=120, out_features=120, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (8): BatchNorm1d(120, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): ReLU()\n",
      "  (10): DynamicQuantizedLinear(in_features=120, out_features=4, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (11): BatchNorm1d(4, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (12): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load your trained model\n",
    "model = torch.load('neural_networks/tmp_benchmark.pt', weights_only=False)\n",
    "print(model[0])\n",
    "\n",
    "# Apply dynamic quantization (works on Linear, LSTM)\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "print(quantized_model)\n",
    "# Save quantized model\n",
    "torch.save({'network_state_dict': model.state_dict()}, 'neural_networks/tmp_benchmark_quantized1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Redbit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mRedbit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuantLinear  \u001b[38;5;66;03m# RedBit layer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load your original model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Redbit'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Redbit.nn import QuantLinear  # RedBit layer\n",
    "from copy import deepcopy\n",
    "\n",
    "# Load your original model\n",
    "model = torch.load('neural_networks/tmp_benchmark.pt', weights_only=False)\n",
    "model.eval()\n",
    "\n",
    "# Replace all Linear layers with QuantLinear\n",
    "def replace_linear_with_quant(module):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.Linear):\n",
    "            quant_layer = QuantLinear(\n",
    "                in_features=child.in_features,\n",
    "                out_features=child.out_features,\n",
    "                bias=child.bias is not None,\n",
    "                bit=8  # <-- 8-bit quantization\n",
    "            )\n",
    "            quant_layer.weight.data = deepcopy(child.weight.data)\n",
    "            if child.bias is not None:\n",
    "                quant_layer.bias.data = deepcopy(child.bias.data)\n",
    "            setattr(module, name, quant_layer)\n",
    "        else:\n",
    "            replace_linear_with_quant(child)\n",
    "\n",
    "replace_linear_with_quant(model)\n",
    "\n",
    "# Save the modified model\n",
    "torch.save(model, 'neural_networks/tmp_benchmark_redbit_quantized.pt')\n",
    "load\n",
    "\n",
    "# Optional: print the quantized model\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quantized_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Use quantized model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mquantized_model\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m loader \u001b[38;5;241m=\u001b[39m test_loader\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quantized_model' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Use quantized model\n",
    "model = quantized_model\n",
    "model.eval()\n",
    "\n",
    "loader = test_loader\n",
    "running_loss = 0\n",
    "\n",
    "loop = tqdm(enumerate(loader), total=len(loader), leave=False)\n",
    "\n",
    "for i, (data, targets) in loop:\n",
    "    data = data.cpu()      # ensure inputs are on CPU\n",
    "    targets = targets.cpu()\n",
    "\n",
    "    outputs = model(data)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "\n",
    "loop.close()\n",
    "print('average loss =', running_loss / len(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total quantized weights (absolute values) in model: 31200\n",
      "\n",
      "Global quantized weight distribution (absolute values) saved to: quantized_weight_analysis_global_abs.csv\n"
     ]
    }
   ],
   "source": [
    "# Hamming distance\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "# Load the quantized model\n",
    "model = torch.load('neural_networks/tmp_benchmark_quantized.pt', weights_only=False)\n",
    "model.eval()\n",
    "\n",
    "# Hamming distance between 8-bit integers\n",
    "def hamming_distance(a, b=0):\n",
    "    return bin(a ^ b).count(\"1\")\n",
    "\n",
    "# Initialize total counter for all layers (using absolute values)\n",
    "global_counter = collections.Counter()\n",
    "global_total = 0\n",
    "\n",
    "# Aggregate all quantized weights (absolute values) across all layers\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.quantized.dynamic.Linear):\n",
    "        weight = module._packed_params._weight_bias()[0]\n",
    "        flat = weight.int_repr().view(-1).cpu().numpy()\n",
    "        flat_abs = abs(flat)  # take absolute values\n",
    "        global_counter.update(flat_abs)\n",
    "        global_total += flat_abs.size\n",
    "\n",
    "print(f\"\\nTotal quantized weights (absolute values) in model: {global_total}\")\n",
    "\n",
    "# Output file\n",
    "output_csv = \"quantized_weight_analysis_global_abs.csv\"\n",
    "with open(output_csv, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)            \n",
    "    writer.writerow([\"Quantized Value (abs)\", \"Count\", \"Percentage\", \"Hamming Distance\", \"Hamming x Percentage\"])\n",
    "\n",
    "    # Values from 0 to 127 since we're using absolute values\n",
    "    for val in range(0, 128):\n",
    "        count = global_counter.get(val, 0)\n",
    "        percentage = (count / global_total) * 100 if global_total > 0 else 0\n",
    "        hamming = hamming_distance(val & 0xFF, 0)  # 8-bit representation of abs value\n",
    "        weighted_hamming = hamming * percentage\n",
    "\n",
    "        writer.writerow([\n",
    "            val,\n",
    "            count,\n",
    "            f\"{percentage:.2f}\",\n",
    "            hamming,\n",
    "            f\"{weighted_hamming:.2f}\"\n",
    "        ])\n",
    "\n",
    "print(f\"\\nGlobal quantized weight distribution (absolute values) saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/home/RedBit/QNN/')  # Adjust path as needed\n",
    "import torch.nn as nn\n",
    "from tools import *\n",
    "\n",
    "class QuantizedModel(nn.Module):\n",
    "    def __init__(self, wbits=4, abits=4):\n",
    "        super(QuantizedModel, self).__init__()\n",
    "        self.abits = abits\n",
    "        self.wbits = wbits\n",
    "\n",
    "        if self.abits == 32:\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            self.act = nn.Hardtanh(inplace=True)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Normalize(mean=X_mean, std=X_std),\n",
    "\n",
    "            QLinear(abits=self.abits, wbits=self.wbits, in_features=16, out_features=120, bias=False),\n",
    "            nn.BatchNorm1d(120, eps=1e-4, momentum=0.1, affine=True),\n",
    "            self.act,\n",
    "\n",
    "            QLinear(abits=self.abits, wbits=self.wbits, in_features=120, out_features=120, bias=False),\n",
    "            nn.BatchNorm1d(120, eps=1e-4, momentum=0.1, affine=True),\n",
    "            self.act,\n",
    "\n",
    "            QLinear(abits=self.abits, wbits=self.wbits, in_features=120, out_features=120, bias=False),\n",
    "            nn.BatchNorm1d(120, eps=1e-4, momentum=0.1, affine=True),\n",
    "            self.act,\n",
    "\n",
    "            QLinear(abits=self.abits, wbits=self.wbits, in_features=120, out_features=4, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/RedBit/QNN/', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/aelarrassi/environments/drone_env/lib/python3.10/site-packages', '/tmp/tmplsdnbq2l']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'QuantizedModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneural_networks/tmp_benchmark.pt\u001b[39m\u001b[38;5;124m'\u001b[39m , weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Quantize to 4-bit weights and activations\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mQuantizedModel\u001b[49m(model)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[1;32m     33\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(quantized_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneural_networks/tmp_benchmark_quantized_int4.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QuantizedModel' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.insert(0,'/home/RedBit/QNN/models/')  # Adjust path as needed\n",
    "# sys.path.remove('/home/RedBit/Baseline/tools/')\n",
    "# Remove wrong path\n",
    "wrong_path = '/home/RedBit/Baseline/tools'\n",
    "if wrong_path in sys.path:\n",
    "    sys.path.remove(wrong_path)\n",
    "\n",
    "# Force correct path at the front\n",
    "correct_path = '/home/RedBit/QNN/'\n",
    "if correct_path in sys.path:\n",
    "    sys.path.remove(correct_path)\n",
    "sys.path.insert(0, correct_path)\n",
    "\n",
    "# Clear cached wrong module\n",
    "if 'tools' in sys.modules:\n",
    "    del sys.modules['tools']\n",
    "print(sys.path)\n",
    "from tools import quantization\n",
    "# from quantization import QLinear\n",
    "# from RedBit.quantizer import quantize_pytorch_model\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = torch.load('neural_networks/tmp_benchmark.pt' , weights_only=False)\n",
    "\n",
    "# Quantize to 4-bit weights and activations\n",
    "quantized_model = QuantizedModel(model)\n",
    "\n",
    "# Save\n",
    "torch.save(quantized_model, 'neural_networks/tmp_benchmark_quantized_int4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'QuantizedModel' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load quantized model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneural_networks/tmp_benchmark_quantized_int4.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m quantized_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Your existing test loop\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/drone_env/lib/python3.10/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1524\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1533\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/environments/drone_env/lib/python3.10/site-packages/torch/serialization.py:2114\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   2113\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 2114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2115\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/environments/drone_env/lib/python3.10/site-packages/torch/serialization.py:2103\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 2103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'QuantizedModel' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "# from quantizer import quantize_pytorch_model\n",
    "import torch\n",
    "# Load quantized model\n",
    "quantized_model = torch.load('neural_networks/tmp_benchmark_quantized_int4.pt', weights_only=False)\n",
    "quantized_model.eval()\n",
    "\n",
    "# Your existing test loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "loader = test_loader\n",
    "loop = tqdm(enumerate(loader), total=len(loader), leave=False)\n",
    "running_loss = 0\n",
    "\n",
    "for i, (data, targets) in loop:\n",
    "    outputs = quantized_model(data)\n",
    "    loss = criterion(outputs, targets)\n",
    "    running_loss += loss.item()\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "\n",
    "loop.close()\n",
    "print('average loss =', running_loss / len(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (drone_env)",
   "language": "python",
   "name": "drone_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
